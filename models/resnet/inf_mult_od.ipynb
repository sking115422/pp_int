{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.structures.instances import Instances\n",
    "from multiprocessing import Process, Manager\n",
    "import gc\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PATHS\n",
    "# Do not use directory with negatives\n",
    "in_path = \"./test_urls.txt\"\n",
    "inf_img_dir = \"./inf_imgs\"\n",
    "res_out_dir = \"./res_out_dir\"\n",
    "vis_out_dir = \"./res_out_dir/vis\"\n",
    "# cat_path = \"/mnt/nis_lab_research/data/elem_cat/cat_neg_27.json\"\n",
    "cat_path = \"/mnt/nis_lab_research/data/elem_cat/cat_neg_10.json\"\n",
    "master_url_dict_path = '/mnt/nis_lab_research/data/top-1m/top-1m-mapping.json'\n",
    "\n",
    "\n",
    "# OBJECT DETECTOR MODELS\n",
    "# # 2 OD\n",
    "od_paths = [\n",
    "    (\"/mnt/nis_lab_research/data/coco_files/agg/class_2_rem/out/far_shah_b1-b5_b8_train_c0/model_final.pth\", 0.50), # small\n",
    "    (\"/mnt/nis_lab_research/data/coco_files/agg/class_2_rem/out/far_shah_b1-b5_b8_train_c1/model_final.pth\", 0.75)  # large\n",
    "]\n",
    "\n",
    "# # 10 OD\n",
    "# od_paths = [\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c0/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c1/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c2/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c3/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c4/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c5/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c6/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c7/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c8/model_final.pth\", .75),\n",
    "#     (\"/mnt/nis_lab_research/data/coco_files/agg/class_1_rem/out/far_shah_b1-b5_b8_train_c9/model_final.pth\", .75)\n",
    "# ]\n",
    "\n",
    "# CLASSIFIER MODEL\n",
    "# # 10 Class Agg + Neg\n",
    "# class_path = \"/mnt/nis_lab_research/data/class_data/pth/far_shah_b1-b5_b8_train_c10_neg/model_final.pth\"\n",
    "class_path = \"/mnt/nis_lab_research/data/class_data/pth/full_train_neg_c10/model_final.pth\"\n",
    "# 27 Class + Neg\n",
    "# class_path = \"/mnt/nis_lab_research/data/class_data/pth/far_shah_b1-b5_b8_train_neg/model_final.pth\"\n",
    "\n",
    "# ADJUSTABLES\n",
    "bg_color = \"white\"\n",
    "# Possibly adjust padding to fixed value in the future\n",
    "padding = 0.15\n",
    "border = 0\n",
    "\n",
    "# Common element that are nested will be denested by default\n",
    "denest = True\n",
    "denest_thold = 0.20\n",
    "\n",
    "keep_clickable_elems_only = True\n",
    "\n",
    "# If True negatively identified cases will be removed before calulating metrics\n",
    "remove_neg = True\n",
    "\n",
    "\n",
    "iou_thold = .5\n",
    "neg_class_name = \"Random\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting CUDA devices as visible\n",
    "cuda_devices = \"0,1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices:\n",
      "  0: NVIDIA GeForce RTX 3090\n",
      "  1: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "available_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "print(\"Available CUDA devices:\")\n",
    "for i, device_name in enumerate(available_devices):\n",
    "    print(f\"  {i}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3373"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soi(str1, start_char, end_char):\n",
    "    str1 = str(str1)\n",
    "    offst = len(start_char)\n",
    "    ind1 = str1.find(start_char)\n",
    "    ind2 = str1.find(end_char)\n",
    "    s_str = str1[ind1+offst:ind2]\n",
    "    return s_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_xywh(b):\n",
    "    x1, y1, x2, y2 = b\n",
    "    x = x1\n",
    "    y = y1\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    return [x, y, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createDataDict (fn, outputs):\n",
    "    img_shape = list(outputs[\"instances\"].image_size)\n",
    "    img_h = int(img_shape[0])\n",
    "    img_w = int(img_shape[1])\n",
    "    ann_list = []\n",
    "\n",
    "    class_list = get_soi(outputs[\"instances\"].pred_classes, \"[\", \"]\").split(\",\")\n",
    "    \n",
    "    if class_list[0] != \"\":\n",
    "\n",
    "        class_list_new = []\n",
    "        for each in class_list:\n",
    "            if each.strip().isdigit():\n",
    "                class_list_new.append(int(each.strip()))\n",
    "            else:\n",
    "                print(f\"Invalid class ID: {each}\")\n",
    "\n",
    "        bbox_list = get_soi(outputs[\"instances\"].pred_boxes, \"[[\", \"]]\").split(\"]\")\n",
    "        bbox_list_new = []\n",
    "        for each in bbox_list:\n",
    "            bbox = re.sub(\"['[,\\n]\", \"\", each).split(\" \")\n",
    "            bbox_new = []\n",
    "            for item in bbox:\n",
    "                if item != \"\":\n",
    "                    bbox_new.append(float(item))\n",
    "            bbox_new = convert_bbox_xywh(bbox_new)\n",
    "            bbox_list_new.append(bbox_new)\n",
    "\n",
    "        for i in range(0, len(class_list)):\n",
    "            # og was \"bbox_mode\": \"<BoxMode.XYWH_ABS: 1>\"\n",
    "            ann_list.append({\"iscrowd\": 0, \"bbox\": bbox_list_new[i], \"category_id\": class_list_new[i], \"bbox_mode\": 0})\n",
    "    \n",
    "    data_dict = {\n",
    "        \"file_name\": fn,\n",
    "        \"height\": img_h,\n",
    "        \"width\": img_w, \n",
    "        \"annotations\": ann_list\n",
    "    }\n",
    " \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(file_path, bounding_box, padding):\n",
    "    \n",
    "    with Image.open(file_path) as img:\n",
    "        \n",
    "        x_min, y_min, width, height = bounding_box\n",
    "\n",
    "        # Calculate padding in pixels\n",
    "        pad_width = int(width * padding)\n",
    "        pad_height = int(height * padding)\n",
    "\n",
    "        # Adjust the bounding box with padding\n",
    "        x_min = max(x_min - pad_width, 0)\n",
    "        y_min = max(y_min - pad_height, 0)\n",
    "        x1 = min(x_min + width + 2 * pad_width, img.width)\n",
    "        y1 = min(y_min + height + 2 * pad_height, img.height)\n",
    "        \n",
    "        cropped_img = img.crop((x_min, y_min, x1, y1))\n",
    "        \n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_to_bg(image, background_color, bg_width, bg_height):\n",
    "    \n",
    "    # Create a new image with the specified background color and dimensions\n",
    "    background = Image.new('RGB', (bg_width, bg_height), background_color)\n",
    "\n",
    "    # Calculate the position to paste the image so it's centered\n",
    "    x = (bg_width - image.width) // 2\n",
    "    y = (bg_height - image.height) // 2\n",
    "\n",
    "    # Paste the image onto the background\n",
    "    background.paste(image, (x, y), image if image.mode == 'RGBA' else None)\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_ar_lock(img, target_size):\n",
    "\n",
    "    original_width, original_height = img.size\n",
    "    target_width, target_height = target_size\n",
    "\n",
    "    # Calculate scaling factor\n",
    "    scaling_factor = min(target_width / original_width, target_height / original_height)\n",
    "\n",
    "    # Calculate new dimensions\n",
    "    new_width = max(int(original_width * scaling_factor), 1)\n",
    "    new_height = max(int(original_height * scaling_factor), 1)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_str(length):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    random_string = ''.join(random.choice(characters) for i in range(length))\n",
    "    return random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_fp, bbox_list, padding, bg_color, border):\n",
    "    \n",
    "    # Create an empty list to store processed images\n",
    "    processed_images = []\n",
    "\n",
    "    for j, bbox in enumerate(bbox_list):\n",
    "\n",
    "        try:\n",
    "            elem_img = crop_image(img_fp, bbox, padding)\n",
    "            e_w = elem_img.size[0]\n",
    "            e_h = elem_img.size[1]\n",
    "\n",
    "            if e_w < e_h:\n",
    "                elem_img = paste_to_bg(elem_img, bg_color, e_h + border, e_h + border)\n",
    "            elif e_w > e_h:\n",
    "                elem_img = paste_to_bg(elem_img, bg_color, e_w + border, e_w + border)\n",
    "                \n",
    "            # elem_img = transform(elem_img)\n",
    "            processed_images.append(elem_img)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(img_fp)\n",
    "            print(e)\n",
    "\n",
    "    # Return the list of processed images\n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(image_path, bbox_list, label_list, output_path, color = 'red', thickness=2):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for bbox, label in zip(bbox_list, label_list):\n",
    "        x, y, w, h = bbox\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=color, width = 2)\n",
    "        draw.text((x, y), label, fill=color, font=font)\n",
    "\n",
    "    # Save the new image\n",
    "    image.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes_in_mem(image, bbox_list, label_list, color='red', thickness=2):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for bbox, label in zip(bbox_list, label_list):\n",
    "        x, y, w, h = bbox\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=color, width=thickness)\n",
    "        draw.text((x, y), label, fill=color, font=font)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_outputs(outputs):\n",
    "    l = []\n",
    "    for out in outputs:\n",
    "        l.append(out[\"instances\"])\n",
    "    new = Instances.cat(l)\n",
    "    return {\"instances\": new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_all_but_first_part(s):\n",
    "    parts = s.split('-')\n",
    "    if len(parts) > 1:\n",
    "        return '-'.join(parts[1:])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(res_out_dir):\n",
    "    shutil.rmtree(res_out_dir)\n",
    "os.makedirs(vis_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/25 01:46:36 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /mnt/nis_lab_research/data/coco_files/agg/class_2_rem/out/far_shah_b1-b5_b8_train_c0/model_final.pth ...\n",
      "\u001b[32m[07/25 01:46:38 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /mnt/nis_lab_research/data/coco_files/agg/class_2_rem/out/far_shah_b1-b5_b8_train_c1/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "obj_det_pred_list = []\n",
    "\n",
    "for od in od_paths:\n",
    "    \n",
    "    setup_logger()\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "    # cfg.MODEL.WEIGHTS = os.path.join(\"/home/dtron2_user/ls_dtron2_full/model/output\", \"model_final.pth\")\n",
    "    cfg.MODEL.WEIGHTS = od[0]\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = od[1] \n",
    "    obj_det_pred = DefaultPredictor(cfg)\n",
    "    obj_det_pred_list.append(obj_det_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = torch.load(class_path)\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cat_path, 'r') as f:\n",
    "    cats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_class_id = \"\"\n",
    "\n",
    "for key, value in cats.items():\n",
    "    if value == neg_class_name:\n",
    "        neg_class_id = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(master_url_dict_path) as f:\n",
    "    master_url_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clickable_elements(domains):\n",
    "\n",
    "    obj_list = []\n",
    "\n",
    "    for dom in domains:\n",
    "\n",
    "        print(dom)\n",
    "\n",
    "        url = f'https://{dom}'\n",
    "\n",
    "        cmd = ['node', '../../get_clickable_elems/get_clickable_elems.js', url]\n",
    "        try:\n",
    "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            print(result.stdout)  # Print the standard output from the command\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing command: {e}\")\n",
    "            print(f\"Command output: {e.output}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        json_path = os.path.join('./data', f'{dom}.json' )\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path) as f:\n",
    "                jobj = json.load(f)\n",
    "\n",
    "            obj_list.append(jobj)\n",
    "\n",
    "            return obj_list\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxA, boxB):\n",
    "    # Convert from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "    x1A, y1A, x2A, y2A = boxA[0], boxA[1], boxA[0] + boxA[2], boxA[1] + boxA[3]\n",
    "    x1B, y1B, x2B, y2B = boxB[0], boxB[1], boxB[0] + boxB[2], boxB[1] + boxB[3]\n",
    "    \n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    xA = max(x1A, x1B)\n",
    "    yA = max(y1A, y1B)\n",
    "    xB = min(x2A, x2B)\n",
    "    yB = min(y2A, y2B)\n",
    "    \n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    # Compute the area of both the prediction and true bounding boxes\n",
    "    boxAArea = (x2A - x1A) * (y2A - y1A)\n",
    "    boxBArea = (x2B - x1B) * (y2B - y1B)\n",
    "    \n",
    "    # Compute the area of union\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "    \n",
    "    # Compute the Intersection over Union by dividing the intersection area by the union area\n",
    "    iou = interArea / unionArea\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_contains(bbox1, bbox2):\n",
    "    return (bbox1[0] <= bbox2[0] and\n",
    "            bbox1[1] <= bbox2[1] and\n",
    "            bbox1[0] + bbox1[2] >= bbox2[0] + bbox2[2] and\n",
    "            bbox1[1] + bbox1[3] >= bbox2[1] + bbox2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_clickable_elem_ind_list(img_path, pred_bbox_list):\n",
    "    # Assuming the existence of keep_all_but_first_part, master_url_dict, get_clickable_elements, and calculate_iou\n",
    "\n",
    "    domain = img_path.split('/')[-1][:-4]\n",
    "\n",
    "    # Get clickable elements for the domain\n",
    "    ces = get_clickable_elements([domain])\n",
    "\n",
    "    if ces is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        ces_obj = ces[0]\n",
    "\n",
    "        # Build list of clickable bounding boxes\n",
    "        clickable_bbox_list = [[ce['x'], ce['y'], ce['width'], ce['height']] for ce in ces_obj]\n",
    "\n",
    "        # List to hold non-clickable element indices\n",
    "        non_clickable_ind_list = []\n",
    "\n",
    "        # Check each predicted bounding box against clickable bounding boxes\n",
    "        for i, p_bbox in enumerate(pred_bbox_list):\n",
    "            is_non_clickable = True\n",
    "            for ces in ces_obj:\n",
    "                ces_bbox = [ces['x'], ces['y'], ces['width'], ces['height']]\n",
    "                iou_score = calculate_iou(p_bbox, ces_bbox)\n",
    "                # if overlap between predicted element and clickable element is greater that threshold\n",
    "                # or if one is contained in the other. It will be kept as clickable predicted element.\n",
    "                if (iou_score >= 0.5 or\n",
    "                    bbox_contains(p_bbox, ces_bbox) or\n",
    "                    bbox_contains(ces_bbox, p_bbox)):\n",
    "                    is_non_clickable = False\n",
    "                    break\n",
    "            if is_non_clickable:\n",
    "                non_clickable_ind_list.append(i)\n",
    "\n",
    "        return non_clickable_ind_list, clickable_bbox_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_search_dict(dictionary, target_value):\n",
    "    for key, value in dictionary.items():\n",
    "        if value == target_value:\n",
    "            return key\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nested_bboxes(bboxes, iou_threshold=0.20):\n",
    "    indices_to_remove = []\n",
    "    \n",
    "    # Create a list of indices\n",
    "    remaining_indices = list(range(len(bboxes)))\n",
    "    \n",
    "    while remaining_indices:\n",
    "        # Get the index and corresponding box of the first element in remaining_indices\n",
    "        current_index = remaining_indices.pop(0)\n",
    "        current_box = bboxes[current_index]\n",
    "        \n",
    "        # List to hold indices of boxes that do not overlap significantly\n",
    "        non_overlapping_indices = []\n",
    "        \n",
    "        for other_index in remaining_indices:\n",
    "            other_box = bboxes[other_index]\n",
    "            iou = calculate_iou(current_box, other_box)\n",
    "            \n",
    "            # if box contains another box that is large enough keep outter box\n",
    "            if (iou > iou_threshold and (bbox_contains(current_box, other_box) or bbox_contains(other_box, current_box))):\n",
    "                if (current_box[2] * current_box[3]) >= (other_box[2] * other_box[3]):\n",
    "                    indices_to_remove.append(other_index)\n",
    "                else:\n",
    "                    indices_to_remove.append(current_index)\n",
    "                    current_box = other_box\n",
    "                    current_index = other_index\n",
    "            # if too much overlap between boxes keep larger box\n",
    "            elif iou > iou_threshold:\n",
    "                if (current_box[2] * current_box[3]) >= (other_box[2] * other_box[3]):\n",
    "                    indices_to_remove.append(other_index)\n",
    "                else:\n",
    "                    indices_to_remove.append(current_index)\n",
    "                    current_box = other_box\n",
    "                    current_index = other_index\n",
    "            else:\n",
    "                non_overlapping_indices.append(other_index)\n",
    "        \n",
    "        # Update the list of remaining indices to check\n",
    "        remaining_indices = non_overlapping_indices\n",
    "    \n",
    "    return sorted(indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async def save_webpage_screenshot(domain_name, save_dir):\n",
    "    try:\n",
    "        url = f'http://{domain_name}'\n",
    "        file_name = f\"{domain_name}.png\"\n",
    "\n",
    "        # Full path for the screenshot file\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "            await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n",
    "            \n",
    "            # Open the webpage\n",
    "            await page.goto(url)\n",
    "            \n",
    "            # Save the screenshot\n",
    "            await page.screenshot(path=file_path)\n",
    "            \n",
    "            await browser.close()\n",
    "\n",
    "        return file_path, None\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "\n",
    "# Wrapper to run the async function\n",
    "def run_screenshot_task(domain_name, save_dir):\n",
    "    return asyncio.run(save_webpage_screenshot(domain_name, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in url list\n",
    "with open(in_path) as f:\n",
    "    domain_list = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating inference imgs directory\n",
    "if os.path.exists(inf_img_dir):\n",
    "    shutil.rmtree(inf_img_dir)\n",
    "    os.makedirs(inf_img_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data directory\n",
    "if os.path.exists(\"./data\"):\n",
    "    shutil.rmtree(\"./data\")\n",
    "    os.makedirs(\"./data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################################\n",
      "0 ./inf_imgs/google.com.png\n",
      "predicted number: 27\n",
      "denested number: 26\n",
      "google.com\n",
      "2024-07-25 01:46:42 [info]: Attempt 1 to process https://google.com\n",
      "2024-07-25 01:46:43 [info]: Navigated to https://google.com\n",
      "2024-07-25 01:46:49 [info]: Successfully processed https://google.com\n",
      "\n",
      "non clickable 0\n",
      "clickable 31\n",
      "cleaned number after removing non clickable elements: 26\n",
      "cleaned number after removing negative classes: 25\n",
      "An error occurred: Page.goto: net::ERR_NAME_NOT_RESOLVED at http://a-msedge.net/\n",
      "Call log:\n",
      "navigating to \"http://a-msedge.net/\", waiting until \"load\"\n",
      "\n",
      "###############################################################################################\n",
      "1) a-msedge.net failed... Skipping\n",
      "###############################################################################################\n",
      "2 ./inf_imgs/youtube.com.png\n",
      "predicted number: 10\n",
      "denested number: 6\n",
      "youtube.com\n"
     ]
    }
   ],
   "source": [
    "master_det_dict = []\n",
    "\n",
    "for i, dom in enumerate(domain_list):\n",
    "\n",
    "    img_path, error = run_screenshot_task(dom, inf_img_dir)\n",
    "\n",
    "    if img_path == None:\n",
    "        print('###############################################################################################')\n",
    "        print(error)\n",
    "        print(f'{i}) {dom} failed... Skipping')\n",
    "        continue\n",
    "\n",
    "    vis_outpath = os.path.join(vis_out_dir, os.path.basename(img_path))\n",
    "\n",
    "    # Creating master dictionary of detected elements\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    pred_out_list = []\n",
    "    for od_pred in obj_det_pred_list:\n",
    "        pred_out_list.append(od_pred(img))\n",
    "    outputs = merge_outputs(pred_out_list)\n",
    "    \n",
    "    print('###############################################################################################')\n",
    "    print(i, img_path)\n",
    "    \n",
    "    data_dict = createDataDict(img_path, outputs)\n",
    "    pred_bbox_list = [ann[\"bbox\"] for ann in data_dict[\"annotations\"]]\n",
    "\n",
    "    print(\"predicted number:\", len(data_dict[\"annotations\"]))\n",
    "    \n",
    "    if denest:\n",
    "        rem_ind_list = filter_nested_bboxes(pred_bbox_list, denest_thold)\n",
    "        for ind in sorted(rem_ind_list, reverse=True):\n",
    "            del data_dict['annotations'][ind]\n",
    "            del pred_bbox_list[ind]\n",
    "\n",
    "    print(\"denested number:\", len(data_dict[\"annotations\"]))\n",
    "\n",
    "    if keep_clickable_elems_only:\n",
    "        nce_list, ce_bbox_list = get_non_clickable_elem_ind_list(img_path, pred_bbox_list)\n",
    "\n",
    "        if nce_list == None and ce_bbox_list == None:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "\n",
    "            print('non clickable', len(nce_list))\n",
    "            print('clickable', len(ce_bbox_list))\n",
    "\n",
    "            z_list = ['0' for _ in range(len(ce_bbox_list))]\n",
    "            \n",
    "            draw_bounding_boxes(img_path, ce_bbox_list, z_list, vis_outpath, color='green')\n",
    "\n",
    "            for ind in sorted(nce_list, reverse=True):\n",
    "                data_dict[\"annotations\"].pop(ind)\n",
    "                pred_bbox_list.pop(ind)\n",
    "\n",
    "            print(\"cleaned number after removing non clickable elements:\", len(data_dict[\"annotations\"]))\n",
    "\n",
    "    elem_img_list = process_image(data_dict[\"file_name\"], pred_bbox_list, padding, bg_color, border)\n",
    "    \n",
    "    pred_ids = []\n",
    "    pred_classes = []\n",
    "    remove_list = []\n",
    "    \n",
    "    for j, img in enumerate(elem_img_list):\n",
    "        \n",
    "        img_t = transform(img.convert('RGB')).unsqueeze(0).to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = classifier(img_t)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        # OG\n",
    "        # pred_class_id = str(predicted.item() + 1)\n",
    "        pred_class_id = str(predicted.item())\n",
    "        pred_class_name = cats[pred_class_id]\n",
    "        \n",
    "        if remove_neg and pred_class_id == neg_class_id:\n",
    "            remove_list.append(j)\n",
    "\n",
    "        pred_ids.append(pred_class_id)\n",
    "        pred_classes.append(pred_classes)\n",
    "        # data_dict[\"annotations\"][j][\"category_id\"] = int(pred_class_id)\n",
    "        data_dict[\"annotations\"][j][\"category_id\"] = pred_class_name\n",
    "    \n",
    "    if remove_neg and remove_list:\n",
    "         for ind in sorted(remove_list, reverse=True):\n",
    "            data_dict[\"annotations\"].pop(ind)\n",
    "            pred_bbox_list.pop(ind)\n",
    "            pred_ids.pop(ind)\n",
    "            pred_classes.pop(ind)\n",
    "            \n",
    "    print(\"cleaned number after removing negative classes:\", len(data_dict[\"annotations\"]))\n",
    "    \n",
    "    if os.path.exists(vis_outpath):\n",
    "        draw_bounding_boxes(vis_outpath, pred_bbox_list, pred_ids, vis_outpath, color='red')\n",
    "    else:\n",
    "        draw_bounding_boxes(img_path, pred_bbox_list, pred_ids, vis_outpath, color='red')\n",
    "    \n",
    "    master_det_dict.append(data_dict)\n",
    "    \n",
    "res_outpath = os.path.join(res_out_dir, \"results.json\")\n",
    "print(\"writing out results to\", res_outpath)\n",
    "with open(res_outpath, 'w+') as f:\n",
    "    json.dump(master_det_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "int_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
